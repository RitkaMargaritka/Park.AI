{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install scikit-image\n",
    "#%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.patches as patches\n",
    "from utils import *\n",
    "from model import *\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import ops\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images\n",
    "\n",
    "The annotations should also contain the corresponding image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionDataset(Dataset):\n",
    "    '''\n",
    "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    images: torch.Tensor of size (B, C, H, W)\n",
    "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
    "    gt classes: torch.Tensor of size (B, max_objects)\n",
    "    '''\n",
    "    def __init__(self, annotation_path, img_dir, img_size, name2idx):\n",
    "        self.annotation_path = annotation_path\n",
    "        self.img_dir = img_dir\n",
    "        self.img_size = img_size\n",
    "        self.name2idx = name2idx\n",
    "        \n",
    "        self.img_data_all, self.gt_bboxes_all, self.gt_classes_all = self.get_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.img_data_all.size(dim=0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.img_data_all[idx], self.gt_bboxes_all[idx], self.gt_classes_all[idx]\n",
    "        \n",
    "    def get_data(self):\n",
    "        img_data_all = []\n",
    "        gt_idxs_all = []\n",
    "        \n",
    "        gt_boxes_all, gt_classes_all, img_paths = parse_annotation(self.annotation_path, self.img_dir, self.img_size)\n",
    "        \n",
    "        for i, img_path in enumerate(img_paths):\n",
    "            \n",
    "            # skip if the image path is not valid\n",
    "            if (not img_path) or (not os.path.exists(img_path)):\n",
    "                continue\n",
    "                \n",
    "            # read and resize image\n",
    "            img = io.imread(img_path)\n",
    "            img = resize(img, self.img_size)\n",
    "            \n",
    "            # convert image to torch tensor and reshape it so channels come first\n",
    "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
    "            \n",
    "            # encode class names as integers\n",
    "            gt_classes = gt_classes_all[i]\n",
    "            gt_idx = torch.Tensor([self.name2idx[name] for name in gt_classes])\n",
    "            \n",
    "            img_data_all.append(img_tensor)\n",
    "            gt_idxs_all.append(gt_idx)\n",
    "        \n",
    "        # pad bounding boxes and classes so they are of the same size\n",
    "        gt_bboxes_pad = pad_sequence(gt_boxes_all, batch_first=True, padding_value=-1)\n",
    "        gt_classes_pad = pad_sequence(gt_idxs_all, batch_first=True, padding_value=-1)\n",
    "        \n",
    "        # stack all images\n",
    "        img_data_stacked = torch.stack(img_data_all, dim=0)\n",
    "        \n",
    "        return img_data_stacked.to(dtype=torch.float32), gt_bboxes_pad, gt_classes_pad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the GT xmls to the format that can be fed to Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_convert_to_cvat(img_path, xml_path, output_filename):\n",
    "    # Create a root element for the merged XML\n",
    "    merged_root = ET.Element('annotations')\n",
    "    xml_files = [ os.path.join(xml_path, x) for x in os.listdir(xml_path)]\n",
    "\n",
    "    # Loop through each XML file\n",
    "    for xml_file in xml_files:\n",
    "        # Parse the XML file\n",
    "        with open(xml_file, 'r') as file:\n",
    "            xml_content = file.read()\n",
    "\n",
    "        xml_root = ET.fromstring(xml_content)\n",
    "        image_filename = os.path.join(img_path, xml_file[-23:-4]+'.jpg')\n",
    "\n",
    "\n",
    "        # Iterate over each 'space' element in the parsed XML and convert to CVAT format\n",
    "        for space_element in xml_root.findall('.//space'):\n",
    "            annotation = ET.Element('annotation')\n",
    "            image_filename_elem = ET.SubElement(annotation, 'filename')\n",
    "            image_filename_elem.text = image_filename\n",
    "\n",
    "            # Convert 'space' element to CVAT format\n",
    "            cvat_box = ET.SubElement(annotation, 'box')\n",
    "            attributes = ET.SubElement(cvat_box, 'attributes')\n",
    "            attribute = ET.SubElement(attributes, 'attribute', {'name': 'occupied'})\n",
    "            attribute.text = space_element.get('occupied')\n",
    "\n",
    "            points = space_element.findall('.//point')\n",
    "            x_values = [int(point.get('x')) for point in points]\n",
    "            y_values = [int(point.get('y')) for point in points]\n",
    "\n",
    "            x_min, x_max = min(x_values), max(x_values)\n",
    "            y_min, y_max = min(y_values), max(y_values)\n",
    "\n",
    "            ET.SubElement(cvat_box, 'xtl').text = str(x_min)\n",
    "            ET.SubElement(cvat_box, 'ytl').text = str(y_min)\n",
    "            ET.SubElement(cvat_box, 'xbr').text = str(x_max)\n",
    "            ET.SubElement(cvat_box, 'ybr').text = str(y_max)\n",
    "\n",
    "            # Append the CVAT annotation to the merged XML\n",
    "            merged_root.append(annotation)\n",
    "\n",
    "    # Create the merged XML tree\n",
    "    merged_tree = ET.ElementTree(merged_root)\n",
    "\n",
    "    # Save the merged XML to a file\n",
    "    with open(output_filename, 'wb') as file:\n",
    "        merged_tree.write(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the .xmls in data/Sunny_most_empty/labels_xml\n",
    "xml_folder_path = '../data/PKLot/Sunny_most_empty/labels_xml'\n",
    "annotation_path = '../data/PKLot/Sunny_most_empty/sunny_data_frcnn.xml'\n",
    "image_dir = '../data/PKLot/Sunny_most_empty/images'\n",
    "\n",
    "merge_and_convert_to_cvat(image_dir, xml_folder_path, annotation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 1280\n",
    "img_height = 720\n",
    "\n",
    "\n",
    "name2idx = {'empty_park': 0, 'occupied_park': 1}\n",
    "idx2name = {v:k for k, v in name2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "received an empty list of sequences",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/thisal_weerasekara/neufische/pklot/notebooks/Faster_R_CNN.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/thisal_weerasekara/neufische/pklot/notebooks/Faster_R_CNN.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m od_dataset \u001b[39m=\u001b[39m ObjectDetectionDataset(annotation_path, image_dir, (img_height, img_width), name2idx)\n",
      "\u001b[1;32m/Users/thisal_weerasekara/neufische/pklot/notebooks/Faster_R_CNN.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thisal_weerasekara/neufische/pklot/notebooks/Faster_R_CNN.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_size \u001b[39m=\u001b[39m img_size\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thisal_weerasekara/neufische/pklot/notebooks/Faster_R_CNN.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname2idx \u001b[39m=\u001b[39m name2idx\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thisal_weerasekara/neufische/pklot/notebooks/Faster_R_CNN.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_data_all, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgt_bboxes_all, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgt_classes_all \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_data()\n",
      "\u001b[1;32m/Users/thisal_weerasekara/neufische/pklot/notebooks/Faster_R_CNN.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thisal_weerasekara/neufische/pklot/notebooks/Faster_R_CNN.ipynb#X13sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     gt_idxs_all\u001b[39m.\u001b[39mappend(gt_idx)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thisal_weerasekara/neufische/pklot/notebooks/Faster_R_CNN.ipynb#X13sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# pad bounding boxes and classes so they are of the same size\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/thisal_weerasekara/neufische/pklot/notebooks/Faster_R_CNN.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m gt_bboxes_pad \u001b[39m=\u001b[39m pad_sequence(gt_boxes_all, batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, padding_value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thisal_weerasekara/neufische/pklot/notebooks/Faster_R_CNN.ipynb#X13sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m gt_classes_pad \u001b[39m=\u001b[39m pad_sequence(gt_idxs_all, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding_value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/thisal_weerasekara/neufische/pklot/notebooks/Faster_R_CNN.ipynb#X13sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# stack all images\u001b[39;00m\n",
      "File \u001b[0;32m~/neufische/pklot/.venv/lib/python3.11/site-packages/torch/nn/utils/rnn.py:400\u001b[0m, in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    396\u001b[0m         sequences \u001b[39m=\u001b[39m sequences\u001b[39m.\u001b[39munbind(\u001b[39m0\u001b[39m)\n\u001b[1;32m    398\u001b[0m \u001b[39m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mpad_sequence(sequences, batch_first, padding_value)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: received an empty list of sequences"
     ]
    }
   ],
   "source": [
    "od_dataset = ObjectDetectionDataset(annotation_path, image_dir, (img_height, img_width), name2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
